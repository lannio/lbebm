{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module, Linear\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import datetime, shutil, argparse, logging, sys\n",
    "\n",
    "import utils\n",
    "# add ddpm\n",
    "def parse_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--seed', default=3, type=int)\n",
    "    parser.add_argument('--gpu_deterministic', type=bool, default=False, help='set cudnn in deterministic mode (slow)')\n",
    "    parser.add_argument(\"--data_scale\", default=60, type=float)\n",
    "    parser.add_argument(\"--dec_size\", default=[1024, 512, 1024], type=list)\n",
    "    parser.add_argument(\"--enc_dest_size\", default=[256, 128], type=list)\n",
    "    parser.add_argument(\"--enc_latent_size\", default=[256, 512], type=list)\n",
    "    parser.add_argument(\"--enc_past_size\", default=[512, 256], type=list)\n",
    "    parser.add_argument(\"--predictor_hidden_size\", default=[1024, 512, 256], type=list)\n",
    "    parser.add_argument(\"--non_local_theta_size\", default=[256, 128, 64], type=list)\n",
    "    parser.add_argument(\"--non_local_phi_size\", default=[256, 128, 64], type=list)\n",
    "    parser.add_argument(\"--non_local_g_size\", default=[256, 128, 64], type=list)\n",
    "    parser.add_argument(\"--non_local_dim\", default=128, type=int)\n",
    "    parser.add_argument(\"--fdim\", default=64, type=int)\n",
    "    parser.add_argument(\"--future_length\", default=12, type=int)\n",
    "    parser.add_argument(\"--device\", default=7, type=int)\n",
    "    parser.add_argument(\"--kld_coeff\", default=0.5, type=float)\n",
    "    parser.add_argument(\"--future_loss_coeff\", default=1, type=float)\n",
    "    parser.add_argument(\"--dest_loss_coeff\", default=2, type=float)\n",
    "    parser.add_argument(\"--learning_rate\", default=0.0001, type=float)\n",
    "    parser.add_argument(\"--lr_decay_step_size\", default=10, type=int)\n",
    "    parser.add_argument(\"--lr_decay_gamma\", default=0.5, type=float)\n",
    "    parser.add_argument(\"--mu\", default=0, type=float)\n",
    "    parser.add_argument(\"--n_values\", default=20, type=int)\n",
    "    parser.add_argument(\"--nonlocal_pools\", default=3, type=int)\n",
    "    parser.add_argument(\"--num_epochs\", default=300, type=int)\n",
    "    parser.add_argument(\"--num_workers\", default=0, type=int)\n",
    "    parser.add_argument(\"--past_length\", default=8, type=int)\n",
    "    parser.add_argument(\"--sigma\", default=1.3, type=float)\n",
    "    parser.add_argument(\"--zdim\", default=64, type=int)\n",
    "    parser.add_argument(\"--print_log\", default=6, type=int)\n",
    "    parser.add_argument(\"--sub_goal_indexes\", default=[2, 5, 8, 11], type=list)\n",
    "\n",
    "\n",
    "    parser.add_argument('--e_prior_sig', type=float, default=2, help='prior of ebm z')\n",
    "    parser.add_argument('--e_init_sig', type=float, default=2, help='sigma of initial distribution')\n",
    "    parser.add_argument('--e_activation', type=str, default='lrelu', choices=['gelu', 'lrelu', 'swish', 'mish'])\n",
    "    parser.add_argument('--e_activation_leak', type=float, default=0.2)\n",
    "    parser.add_argument('--e_energy_form', default='identity', choices=['identity', 'tanh', 'sigmoid', 'softplus'])\n",
    "    parser.add_argument('--e_l_steps', type=int, default=20, help='number of langevin steps')\n",
    "    parser.add_argument('--e_l_steps_pcd', type=int, default=20, help='number of langevin steps')\n",
    "    parser.add_argument('--e_l_step_size', type=float, default=0.4, help='stepsize of langevin')\n",
    "    parser.add_argument('--e_l_with_noise', default=True, type=bool, help='noise term of langevin')\n",
    "    parser.add_argument('--e_sn', default=False, type=bool, help='spectral regularization')\n",
    "    parser.add_argument('--e_lr', default=0.00003, type=float)\n",
    "    parser.add_argument('--e_is_grad_clamp', type=bool, default=False, help='whether doing the gradient clamp')\n",
    "    parser.add_argument('--e_max_norm', type=float, default=25, help='max norm allowed')\n",
    "    parser.add_argument('--e_decay', default=1e-4, help='weight decay for ebm')\n",
    "    parser.add_argument('--e_gamma', default=0.998, help='lr decay for ebm')\n",
    "    parser.add_argument('--e_beta1', default=0.9, type=float)\n",
    "    parser.add_argument('--e_beta2', default=0.999, type=float)\n",
    "    parser.add_argument('--memory_size', default=200000, type=int)\n",
    "\n",
    "\n",
    "    parser.add_argument('--dataset_name', type=str, default='eth')\n",
    "    parser.add_argument('--save_folder', type=str, default='read/')\n",
    "    parser.add_argument('--save_config', type=str, default='demo/')\n",
    "    parser.add_argument('--dataset_folder', type=str, default='dataset')\n",
    "    parser.add_argument('--obs',type=int,default=8)\n",
    "    parser.add_argument('--preds',type=int,default=12)\n",
    "    parser.add_argument('--delim',type=str,default='\\t')\n",
    "    parser.add_argument('--verbose',action='store_true')\n",
    "    parser.add_argument('--val_size',type=int, default=0)\n",
    "    parser.add_argument('--batch_size',type=int,default=70)\n",
    "\n",
    "    parser.add_argument('--ny', type=int, default=1)\n",
    "    # parser.add_argument('--model_path', type=str, default='saved_models/lbebm_eth.pt')\n",
    "    parser.add_argument('--model_path', type=str, default=None)\n",
    "\n",
    "\n",
    "    return parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "def set_gpu(gpu):\n",
    "    torch.cuda.set_device('cuda:{}'.format(gpu))\n",
    "\n",
    "def get_exp_id(file):\n",
    "    return os.path.splitext(os.path.basename(file))[0]\n",
    "\n",
    "def get_output_dir(exp_id):\n",
    "    t = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    output_dir = os.path.join('output/' + exp_id, t)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "def setup_logging(name, output_dir, console=True):\n",
    "    log_format = logging.Formatter(\"%(asctime)s : %(message)s\")\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.handlers = []\n",
    "    output_file = os.path.join(output_dir, 'output.log')\n",
    "    file_handler = logging.FileHandler(output_file)\n",
    "    file_handler.setFormatter(log_format)\n",
    "    logger.addHandler(file_handler)\n",
    "    if console:\n",
    "        console_handler = logging.StreamHandler(sys.stdout)\n",
    "        console_handler.setFormatter(log_format)\n",
    "        logger.addHandler(console_handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    return logger\n",
    "\n",
    "def set_cuda(deterministic=True):\n",
    "    if torch.cuda.is_available():\n",
    "        if not deterministic:\n",
    "            torch.backends.cudnn.deterministic = False\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        else:\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def set_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def copy_source(file, output_dir):\n",
    "    shutil.copyfile(file, os.path.join(output_dir, os.path.basename(file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-04 22:16:56,141 : Namespace(batch_size=70, data_scale=60, dataset_folder='dataset', dataset_name='eth', dec_size=[1024, 512, 1024], delim='\\t', dest_loss_coeff=2, device=7, e_activation='lrelu', e_activation_leak=0.2, e_beta1=0.9, e_beta2=0.999, e_decay=0.0001, e_energy_form='identity', e_gamma=0.998, e_init_sig=2, e_is_grad_clamp=False, e_l_step_size=0.4, e_l_steps=20, e_l_steps_pcd=20, e_l_with_noise=True, e_lr=3e-05, e_max_norm=25, e_prior_sig=2, e_sn=False, enc_dest_size=[256, 128], enc_latent_size=[256, 512], enc_past_size=[512, 256], fdim=64, future_length=12, future_loss_coeff=1, gpu_deterministic=False, kld_coeff=0.5, learning_rate=0.0001, lr_decay_gamma=0.5, lr_decay_step_size=10, memory_size=200000, model_path=None, mu=0, n_values=20, non_local_dim=128, non_local_g_size=[256, 128, 64], non_local_phi_size=[256, 128, 64], non_local_theta_size=[256, 128, 64], nonlocal_pools=3, num_epochs=300, num_workers=0, ny=1, obs=8, past_length=8, predictor_hidden_size=[1024, 512, 256], preds=12, print_log=6, save_config='demo/', save_folder='read/', seed=3, sigma=1.3, sub_goal_indexes=[2, 5, 8, 11], val_size=0, verbose=False, way_points=[0, 1, 3, 4, 6, 7, 9, 10], zdim=64)\n",
      "start loading dataset\n",
      "validation set size -> 0\n",
      "loaded preprocessed data file dataset/eth/train/saved_data.pickle\n",
      "start loading dataset\n",
      "validation set size -> 0\n",
      "loaded preprocessed data file dataset/eth/val/saved_data.pickle\n",
      "start loading dataset\n",
      "validation set size -> 0\n",
      "loaded preprocessed data file dataset/eth/test/saved_data.pickle\n"
     ]
    }
   ],
   "source": [
    "    args = parse_args()\n",
    "    # custom\n",
    "    # if(args.dataset_name=='eth'):\n",
    "    #     args.seed=3\n",
    "    #     args.kld_coeff=0.5\n",
    "    #     args.lr_decay_step_size=10\n",
    "    # elif(args.dataset_name=='hotel'):\n",
    "    #     args.seed=2\n",
    "    #     args.kld_coeff=0.8\n",
    "    #     args.lr_decay_step_size=30\n",
    "    # elif(args.dataset_name=='univ'): \n",
    "    #     args.seed=1\n",
    "    #     args.kld_coeff=0.5\n",
    "    #     args.lr_decay_step_size=30\n",
    "    # elif(args.dataset_name=='zara1'): \n",
    "    #     args.seed=1\n",
    "    #     args.kld_coeff=0.5\n",
    "    #     args.lr_decay_step_size=30\n",
    "    # elif(args.dataset_name=='zara2'): \n",
    "    #     args.seed=1\n",
    "    #     args.kld_coeff=0.5\n",
    "    #     args.lr_decay_step_size=30\n",
    "    \n",
    "    output_dir='/home/yaoliu/scratch/experiment/lbebm/'+args.save_folder + args.save_config + args.dataset_name\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    # copy_source(__file__, output_dir)\n",
    "    set_gpu(args.device)\n",
    "    set_cuda(deterministic=args.gpu_deterministic)\n",
    "    set_seed(args.seed)\n",
    "    args.way_points = list(set(list(range(args.future_length))) - set(args.sub_goal_indexes))\n",
    "\n",
    "    logger = setup_logging('job{}'.format(0), output_dir, console=True)\n",
    "    logger.info(args)\n",
    "\n",
    "    if args.val_size==0:\n",
    "        train_dataset, _ = utils.create_dataset(args.dataset_folder,args.dataset_name,0,args.obs,args.preds,delim=args.delim,train=True, verbose=True)\n",
    "        val_dataset, _ = utils.create_dataset(args.dataset_folder,args.dataset_name,0,args.obs,args.preds,delim=args.delim,train=False, verbose=True)\n",
    "    else:\n",
    "        train_dataset, val_dataset = utils.create_dataset(args.dataset_folder, args.dataset_name, args.val_size,args.obs, args.preds, delim=args.delim, train=True, verbose=args.verbose)\n",
    "\n",
    "    test_dataset, _ =  utils.create_dataset(args.dataset_folder,args.dataset_name,0,args.obs,args.preds,delim=args.delim,train=False,eval=True, verbose=True)\n",
    "\n",
    "    tr_dl = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
    "    val_dl = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size*10, shuffle=False, num_workers=0)\n",
    "    test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size*10, shuffle=False, num_workers=0)\n",
    "\n",
    "    \n",
    "    def initial_pos(traj_batches):\n",
    "        batches = []\n",
    "        for b in traj_batches:\n",
    "            starting_pos = b[:,7,:].copy()/1000\n",
    "            batches.append(starting_pos)\n",
    "        return batches\n",
    "\n",
    "    def sample_p_0(n, nz=16):\n",
    "        return args.e_init_sig * torch.randn(*[n, nz]).double().cuda()\n",
    "\n",
    "    def calculate_loss(dest, dest_recon, mean, log_var, criterion, future, interpolated_future, sub_goal_indexes):\n",
    "        dest_loss = criterion(dest, dest_recon)\n",
    "        future_loss = criterion(future, interpolated_future)\n",
    "        subgoal_reg = criterion(dest_recon, interpolated_future.view(dest.size(0), future.size(1)//2, 2)[:, sub_goal_indexes, :].view(dest.size(0), -1))\n",
    "        kl = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "        return dest_loss, future_loss, kl, subgoal_reg\n",
    "\n",
    "    class PositionalEncoding(nn.Module):\n",
    "        def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "            super().__init__()\n",
    "\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "            pe = torch.zeros(max_len, d_model)\n",
    "            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "            div_term = torch.exp(\n",
    "                torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "            )\n",
    "            pe[:, 0::2] = torch.sin(position * div_term)\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "            pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "            self.register_buffer(\"pe\", pe)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x + self.pe[: x.size(0), :]\n",
    "            return self.dropout(x)\n",
    "\n",
    "    class ConcatSquashLinear(nn.Module):\n",
    "        def __init__(self, dim_in, dim_out, dim_ctx):\n",
    "            super(ConcatSquashLinear, self).__init__()\n",
    "            self._layer = Linear(dim_in, dim_out)\n",
    "            self._hyper_bias = Linear(dim_ctx, dim_out, bias=False)\n",
    "            self._hyper_gate = Linear(dim_ctx, dim_out)\n",
    "\n",
    "        def forward(self, ctx, x):\n",
    "            gate = torch.sigmoid(self._hyper_gate(ctx))\n",
    "            bias = self._hyper_bias(ctx)\n",
    "            # if x.dim() == 3:\n",
    "            #     gate = gate.unsqueeze(1)\n",
    "            #     bias = bias.unsqueeze(1)\n",
    "            ret = self._layer(x) * gate + bias\n",
    "            return ret        \n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, hidden_size=(1024, 512), activation='relu', discrim=False, dropout=-1):\n",
    "            super(MLP, self).__init__()\n",
    "            dims = []\n",
    "            dims.append(input_dim)\n",
    "            dims.extend(hidden_size)\n",
    "            dims.append(output_dim)\n",
    "            self.layers = nn.ModuleList()\n",
    "            for i in range(len(dims)-1):\n",
    "                self.layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "\n",
    "            if activation == 'relu':\n",
    "                self.activation = nn.ReLU()\n",
    "            elif activation == 'sigmoid':\n",
    "                self.activation = nn.Sigmoid()\n",
    "\n",
    "            self.sigmoid = nn.Sigmoid() if discrim else None\n",
    "            self.dropout = dropout\n",
    "\n",
    "        def forward(self, x):\n",
    "            for i in range(len(self.layers)):\n",
    "                x = self.layers[i](x)\n",
    "                if i != len(self.layers)-1:\n",
    "                    x = self.activation(x)\n",
    "                    if self.dropout != -1:\n",
    "                        x = nn.Dropout(min(0.1, self.dropout/3) if i == 1 else self.dropout)(x)\n",
    "                elif self.sigmoid:\n",
    "                    x = self.sigmoid(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    class ReplayMemory(object):\n",
    "        def __init__(self, capacity):\n",
    "            self.capacity = capacity\n",
    "            self.memory = []\n",
    "            self.position = 0\n",
    "\n",
    "        def push(self, input_memory):\n",
    "            if len(self.memory) < self.capacity:\n",
    "                self.memory.append(None)\n",
    "            self.memory[self.position] = input_memory\n",
    "            self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "        def sample(self, n=100):\n",
    "            samples = random.sample(self.memory, n)\n",
    "            return torch.cat(samples)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.memory)\n",
    "\n",
    "    def extract(a, t, x_shape):\n",
    "        batch_size = t.shape[0]\n",
    "        out = a.to(t.device).gather(0, t).float()\n",
    "        out = out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
    "        return out\n",
    "\n",
    "\n",
    "    class LBEBM(nn.Module):\n",
    "        def __init__(self, \n",
    "                    enc_past_size, \n",
    "                    enc_dest_size, \n",
    "                    enc_latent_size, \n",
    "                    dec_size, \n",
    "                    predictor_size, \n",
    "                    fdim, \n",
    "                    zdim, \n",
    "                    sigma, \n",
    "                    past_length, \n",
    "                    future_length):\n",
    "            super(LBEBM, self).__init__()\n",
    "\n",
    "            # var_sched\n",
    "            self.num_steps = 10\n",
    "            self.beta_1 = 1e-4\n",
    "            self.beta_T = 5e-2\n",
    "            self.mode = 'linear'\n",
    "            self.cosine_s=8e-3\n",
    "\n",
    "            if self.mode == 'linear':\n",
    "                betas = torch.linspace(self.beta_1, self.beta_T, steps=self.num_steps)\n",
    "            elif self.mode == 'cosine':\n",
    "                timesteps = (\n",
    "                torch.arange(self.num_steps + 1) / self.num_steps + self.cosine_s\n",
    "                )\n",
    "                alphas = timesteps / (1 + self.cosine_s) * math.pi / 2\n",
    "                alphas = torch.cos(alphas).pow(2)\n",
    "                alphas = alphas / alphas[0]\n",
    "                betas = 1 - alphas[1:] / alphas[:-1]\n",
    "                betas = betas.clamp(max=0.999)\n",
    "\n",
    "            betas = torch.cat([torch.zeros([1]), betas], dim=0)     # Padding\n",
    "\n",
    "            alphas = 1 - betas\n",
    "            log_alphas = torch.log(alphas)\n",
    "            for i in range(1, log_alphas.size(0)):  # 1 to T\n",
    "                log_alphas[i] += log_alphas[i - 1]\n",
    "            alpha_bars = log_alphas.exp()\n",
    "\n",
    "            sigmas_flex = torch.sqrt(betas)\n",
    "            sigmas_inflex = torch.zeros_like(sigmas_flex)\n",
    "            for i in range(1, sigmas_flex.size(0)):\n",
    "                sigmas_inflex[i] = ((1 - alpha_bars[i-1]) / (1 - alpha_bars[i])) * betas[i]\n",
    "            sigmas_inflex = torch.sqrt(sigmas_inflex)\n",
    "\n",
    "            self.register_buffer('betas', betas)\n",
    "            self.register_buffer('alphas', alphas)\n",
    "            self.register_buffer('alpha_bars', alpha_bars)\n",
    "            self.register_buffer('sigmas_flex', sigmas_flex)\n",
    "            self.register_buffer('sigmas_inflex', sigmas_inflex)\n",
    "\n",
    "            # backbone\n",
    "            # point_dim=2\n",
    "            context_dim=256\n",
    "            tf_layer=3\n",
    "            residual=False\n",
    "            d_model=16\n",
    "            self.residual = residual\n",
    "            self.pos_emb = PositionalEncoding(d_model=d_model, dropout=0.1, max_len=20)\n",
    "            self.concat1 = ConcatSquashLinear(zdim,2*zdim,fdim+3)\n",
    "            self.layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=4, dim_feedforward=zdim)\n",
    "            self.transformer_encoder = nn.TransformerEncoder(self.layer, num_layers=tf_layer)\n",
    "            self.concat3 = ConcatSquashLinear(2*zdim,4*zdim,fdim+3)\n",
    "            self.concat4 = ConcatSquashLinear(4*zdim,2*zdim,fdim+3)\n",
    "            self.linear = ConcatSquashLinear(2*zdim, zdim, fdim+3)\n",
    "\n",
    "\n",
    "            self.zdim = zdim\n",
    "            self.sigma = sigma\n",
    "            self.nonlocal_pools = args.nonlocal_pools\n",
    "            non_local_dim = args.non_local_dim\n",
    "            non_local_phi_size = args.non_local_phi_size\n",
    "            non_local_g_size = args.non_local_g_size\n",
    "            non_local_theta_size = args.non_local_theta_size\n",
    "\n",
    "            self.encoder_past = MLP(input_dim=past_length*2, output_dim=fdim, hidden_size=enc_past_size)\n",
    "            self.encoder_dest = MLP(input_dim=len(args.sub_goal_indexes)*2, output_dim=fdim, hidden_size=enc_dest_size)\n",
    "            self.encoder_latent = MLP(input_dim=2*fdim, output_dim=2*zdim, hidden_size=enc_latent_size)\n",
    "            self.decoder = MLP(input_dim=fdim+zdim, output_dim=len(args.sub_goal_indexes)*2, hidden_size=dec_size)\n",
    "            self.predictor = MLP(input_dim=2*fdim, output_dim=2*(future_length), hidden_size=predictor_size)\n",
    "\n",
    "            self.non_local_theta = MLP(input_dim = fdim, output_dim = non_local_dim, hidden_size=non_local_theta_size)\n",
    "            self.non_local_phi = MLP(input_dim = fdim, output_dim = non_local_dim, hidden_size=non_local_phi_size)\n",
    "            self.non_local_g = MLP(input_dim = fdim, output_dim = fdim, hidden_size=non_local_g_size)\n",
    "\n",
    "            self.EBM = nn.Sequential(\n",
    "                nn.Linear(zdim + fdim, 200),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(200, 200),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(200, args.ny),\n",
    "                )\n",
    "                        \n",
    "            self.replay_memory = ReplayMemory(args.memory_size)\n",
    "\n",
    "        def forward(self, x, dest=None, mask=None, iteration=1, y=None):\n",
    "            # x torch.Size([70, 16])\n",
    "            ftraj = self.encoder_past(x) # torch.Size([70, 16])\n",
    "\n",
    "            if mask:\n",
    "                for _ in range(self.nonlocal_pools):\n",
    "                    ftraj = self.non_local_social_pooling(ftraj, mask)\n",
    "\n",
    "            if self.training:\n",
    "                dest_features = self.encoder_dest(dest) # torch.Size([70, 16])\n",
    "                features = torch.cat((ftraj, dest_features), dim=1) # torch.Size([70, 32])\n",
    "                latent =  self.encoder_latent(features) # torch.Size([70, 32])\n",
    "                mu = latent[:, 0:self.zdim]\n",
    "                logvar = latent[:, self.zdim:]\n",
    "\n",
    "                var = logvar.mul(0.5).exp_()\n",
    "                eps = torch.DoubleTensor(var.size()).normal_().cuda()\n",
    "                z_g_k = eps.mul(var).add_(mu)\n",
    "                z_g_k = z_g_k.double().cuda() # torch.Size([70, 16])\n",
    "\n",
    "            if self.training:\n",
    "                # pcd = True if len(self.replay_memory) == args.memory_size else False\n",
    "                # if pcd:\n",
    "                #     z_e_0 = self.replay_memory.sample(n=ftraj.size(0)).clone().detach().cuda()\n",
    "                # else:\n",
    "                #     z_e_0 = sample_p_0(n=ftraj.size(0), nz=self.zdim)\n",
    "                # z_e_k, _ = self.sample_langevin_prior_z(Variable(z_e_0), ftraj, pcd=pcd, verbose=(iteration % 1000==0))\n",
    "                # for _z_e_k in z_e_k.clone().detach().cpu().split(1):\n",
    "                #     self.replay_memory.push(_z_e_k)\n",
    "                z_e_k = z_g_k\n",
    "            else:\n",
    "                # z_e_0 = sample_p_0(n=ftraj.size(0), nz=self.zdim) # torch.Size([70, 16])\n",
    "                z_e_k = self.diffusion_sample_ddpm(ftraj)\n",
    "                # z_e_k, _ = self.sample_langevin_prior_z(Variable(z_e_0), ftraj, pcd=False, verbose=(iteration % 1000==0), y=y)                        \n",
    "            z_e_k = z_e_k.double().cuda()\n",
    "\n",
    "            if self.training:\n",
    "                decoder_input = torch.cat((ftraj, z_g_k), dim=1)\n",
    "            else:\n",
    "                decoder_input = torch.cat((ftraj, z_e_k), dim=1)\n",
    "            generated_dest = self.decoder(decoder_input)\n",
    "\n",
    "            if self.training:\n",
    "                generated_dest_features = self.encoder_dest(generated_dest)\n",
    "                prediction_features = torch.cat((ftraj, generated_dest_features), dim=1)\n",
    "                pred_future = self.predictor(prediction_features)\n",
    "\n",
    "                # en_pos = self.ebm(z_g_k, ftraj).mean() # torch.Size([70]) mean\n",
    "                # en_neg = self.ebm(z_e_k.detach().clone(), ftraj).mean()\n",
    "                # cd = en_pos - en_neg\n",
    "                cd = self.diffusion_loss(z_g_k, ftraj)\n",
    "                return generated_dest, mu, logvar, pred_future, cd#, en_pos, en_neg, pcd\n",
    "\n",
    "            return generated_dest\n",
    "\n",
    "        def diffusion_loss(self, x_0, context, t=None):\n",
    "            # x_0 70,16\n",
    "\n",
    "            batch_size, point_dim = x_0.size()\n",
    "            if t == None:\n",
    "                t = self.uniform_sample_t(batch_size)\n",
    "\n",
    "            alpha_bar = self.alpha_bars[t]\n",
    "            beta = self.betas[t].cuda()\n",
    "\n",
    "            c0 = torch.sqrt(alpha_bar).view(-1, 1).cuda()       # (B, 1, 1)\n",
    "            c1 = torch.sqrt(1 - alpha_bar).view(-1, 1).cuda()   # (B, 1, 1)\n",
    "\n",
    "            e_rand = torch.randn_like(x_0).cuda()  # (B, N, d)\n",
    "\n",
    "\n",
    "            e_theta = self.net(c0 * x_0 + c1 * e_rand, beta=beta, context=context)\n",
    "            loss = F.mse_loss(e_theta.view(-1, point_dim), e_rand.view(-1, point_dim), reduction='mean')\n",
    "            return loss\n",
    "\n",
    "        def diffusion_sample(self, context):\n",
    "\n",
    "            self.alphas_cumprod = self.alpha_bars\n",
    "\n",
    "            batch_size = context.size(0)\n",
    "\n",
    "            ddim_timesteps=10\n",
    "            ddim_eta=0.0\n",
    "            clip_denoised=False\n",
    "\n",
    "            c = self.num_steps // ddim_timesteps\n",
    "            ddim_timestep_seq = np.asarray(list(range(0, self.num_steps, c)))\n",
    "            # add one to get the final alpha values right (the ones from first scale to data during sampling)\n",
    "            ddim_timestep_seq = ddim_timestep_seq + 1\n",
    "            # previous sequence\n",
    "            ddim_timestep_prev_seq = np.append(np.array([0]), ddim_timestep_seq[:-1])\n",
    "\n",
    "            # sample_img = torch.randn([batch_size, self.zdim]).to(context.device)\n",
    "            sample_img = sample_p_0(n=context.size(0), nz=self.zdim)\n",
    "\n",
    "\n",
    "            for i in reversed(range(0, ddim_timesteps)) :\n",
    "                t = torch.full((batch_size,), ddim_timestep_seq[i], device=context.device, dtype=torch.long)\n",
    "                prev_t = torch.full((batch_size,), ddim_timestep_prev_seq[i], device=context.device, dtype=torch.long)\n",
    "                \n",
    "                # 1. get current and previous alpha_cumprod\n",
    "                \n",
    "                alpha_cumprod_t = extract(self.alphas_cumprod, t, sample_img.shape)\n",
    "                alpha_cumprod_t_prev = extract(self.alphas_cumprod, prev_t, sample_img.shape)\n",
    "        \n",
    "                # 2. predict noise using model\n",
    "                beta = self.betas[[t[0].item()]*batch_size]\n",
    "                pred_noise = self.net(sample_img, beta=beta, context=context)\n",
    "                \n",
    "                # 3. get the predicted x_0\n",
    "                pred_x0 = (sample_img - torch.sqrt((1. - alpha_cumprod_t)) * pred_noise) / torch.sqrt(alpha_cumprod_t)\n",
    "                if clip_denoised:\n",
    "                    pred_x0 = torch.clamp(pred_x0, min=-1., max=1.)\n",
    "                \n",
    "                # 4. compute variance: \"sigma_t(η)\" -> see formula (16)\n",
    "                # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)\n",
    "                sigmas_t = ddim_eta * torch.sqrt(\n",
    "                    (1 - alpha_cumprod_t_prev) / (1 - alpha_cumprod_t) * (1 - alpha_cumprod_t / alpha_cumprod_t_prev))\n",
    "                \n",
    "                # 5. compute \"direction pointing to x_t\" of formula (12)\n",
    "                pred_dir_xt = torch.sqrt(1 - alpha_cumprod_t_prev - sigmas_t**2) * pred_noise\n",
    "                \n",
    "                # 6. compute x_{t-1} of formula (12)\n",
    "                x_prev = torch.sqrt(alpha_cumprod_t_prev) * pred_x0 + pred_dir_xt + sigmas_t * torch.randn_like(sample_img)\n",
    "\n",
    "                sample_img = x_prev.detach()\n",
    "\n",
    "            return sample_img\n",
    "\n",
    "\n",
    "        def diffusion_sample_ddpm(self, context):\n",
    "            batch_size = context.size(0)\n",
    "\n",
    "            x_T = sample_p_0(n=context.size(0), nz=self.zdim)\n",
    "\n",
    "            traj = {self.num_steps: x_T} # {100:xt}\n",
    "            for t in range(self.num_steps, 0, -1):\n",
    "                z = torch.randn_like(x_T) if t > 1 else torch.zeros_like(x_T)\n",
    "                alpha = self.alphas[t]\n",
    "                alpha_bar = self.alpha_bars[t]\n",
    "                sigma = self.get_sigmas(t, 0.0)\n",
    "\n",
    "                c0 = 1.0 / torch.sqrt(alpha)\n",
    "                c1 = (1 - alpha) / torch.sqrt(1 - alpha_bar)\n",
    "\n",
    "                x_t = traj[t]\n",
    "                beta = self.betas[[t]*batch_size]\n",
    "                e_theta = self.net(x_t, beta=beta, context=context)\n",
    "                x_next = c0 * (x_t - c1 * e_theta) + sigma * z\n",
    "                traj[t-1] = x_next.detach()     # Stop gradient and save trajectory.\n",
    "                traj[t] = traj[t].cpu()         # Move previous output to CPU memory.\n",
    "                del traj[t]\n",
    "\n",
    "            return traj[0]\n",
    "\n",
    "        def net(self, x, beta, context):\n",
    "            batch_size = x.size(0)\n",
    "            beta = beta.view(batch_size, 1)          # (B, 1, 1)\n",
    "            context = context.view(batch_size, -1)   # (B, 1, F)\n",
    "\n",
    "            time_emb = torch.cat([beta, torch.sin(beta), torch.cos(beta)], dim=-1)  # (B, 1, 3)\n",
    "            ctx_emb = torch.cat([time_emb, context], dim=-1)    # (B, 1, F+3)\n",
    "            x = self.concat1(ctx_emb,x)\n",
    "            final_emb = x.reshape(x.size()[0],8,-1).permute(1,0,2)\n",
    "\n",
    "            final_emb = self.pos_emb(final_emb)\n",
    "\n",
    "            trans = self.transformer_encoder(final_emb).permute(1,0,2).reshape(x.size()[0],-1)\n",
    "\n",
    "            trans = self.concat3(ctx_emb, trans)\n",
    "            trans = self.concat4(ctx_emb, trans)\n",
    "            return self.linear(ctx_emb, trans)\n",
    "\n",
    "        def uniform_sample_t(self, batch_size):\n",
    "            ts = np.random.choice(np.arange(1, self.num_steps+1), batch_size)\n",
    "            return ts.tolist()\n",
    "\n",
    "        def get_sigmas(self, t, flexibility):\n",
    "            assert 0 <= flexibility and flexibility <= 1\n",
    "            sigmas = self.sigmas_flex[t] * flexibility + self.sigmas_inflex[t] * (1 - flexibility)\n",
    "            return sigmas\n",
    "\n",
    "        def ebm(self, z, condition, cls_output=False):\n",
    "            condition_encoding = condition.detach().clone()\n",
    "            z_c = torch.cat((z, condition_encoding), dim=1)\n",
    "            conditional_neg_energy = self.EBM(z_c)\n",
    "            assert conditional_neg_energy.shape == (z.size(0), args.ny)\n",
    "            if cls_output:\n",
    "                return - conditional_neg_energy\n",
    "            else:\n",
    "                return - conditional_neg_energy.logsumexp(dim=1)\n",
    "        \n",
    "        def sample_langevin_prior_z(self, z, condition, pcd=False, verbose=False, y=None):\n",
    "            z = z.clone().detach()\n",
    "            z.requires_grad = True\n",
    "            _e_l_steps = args.e_l_steps_pcd if pcd else args.e_l_steps\n",
    "            _e_l_step_size = args.e_l_step_size\n",
    "            for i in range(_e_l_steps):\n",
    "                if y is None:\n",
    "                    en = self.ebm(z, condition)\n",
    "                else:\n",
    "                    en = self.ebm(z, condition, cls_output=True)[range(z.size(0)), y]\n",
    "                z_grad = torch.autograd.grad(en.sum(), z)[0]\n",
    "\n",
    "                z.data = z.data - 0.5 * _e_l_step_size * _e_l_step_size * (z_grad + 1.0 / (args.e_prior_sig * args.e_prior_sig) * z.data)\n",
    "                if args.e_l_with_noise:\n",
    "                    z.data += _e_l_step_size * torch.randn_like(z).data\n",
    "\n",
    "                if (i % 5 == 0 or i == _e_l_steps - 1) and verbose:\n",
    "                    if y is None:\n",
    "                        print('Langevin prior {:3d}/{:3d}: energy={:8.3f}'.format(i+1, _e_l_steps, en.sum().item()))\n",
    "                    else:\n",
    "                        logger.info('Conditional Langevin prior {:3d}/{:3d}: energy={:8.3f}'.format(i + 1, _e_l_steps, en.sum().item()))\n",
    "\n",
    "                z_grad_norm = z_grad.view(z_grad.size(0), -1).norm(dim=1).mean()\n",
    "\n",
    "            return z.detach(), z_grad_norm\n",
    "\n",
    "\n",
    "        def predict(self, past, generated_dest):\n",
    "            ftraj = self.encoder_past(past)\n",
    "            generated_dest_features = self.encoder_dest(generated_dest)\n",
    "            prediction_features = torch.cat((ftraj, generated_dest_features), dim=1)\n",
    "            interpolated_future = self.predictor(prediction_features)\n",
    "\n",
    "            return interpolated_future\n",
    "\n",
    "\n",
    "        def non_local_social_pooling(self, feat, mask):\n",
    "            theta_x = self.non_local_theta(feat)\n",
    "            phi_x = self.non_local_phi(feat).transpose(1,0)\n",
    "            f = torch.matmul(theta_x, phi_x)\n",
    "            f_weights = F.softmax(f, dim = -1)\n",
    "            f_weights = f_weights * mask\n",
    "            f_weights = F.normalize(f_weights, p=1, dim=1)\n",
    "            pooled_f = torch.matmul(f_weights, self.non_local_g(feat))\n",
    "\n",
    "            return pooled_f + feat\n",
    "\n",
    "    \n",
    "    def train(model, optimizer, epoch, sub_goal_indexes):\n",
    "        model.train()\n",
    "        train_loss, total_dest_loss, total_future_loss = 0, 0, 0\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        for i, trajx in enumerate(tr_dl):\n",
    "            x = trajx['src'][:, :, :2]\n",
    "            y = trajx['trg'][:, :, :2]\n",
    "            x = x - trajx['src'][:, -1:, :2]\n",
    "            y = y - trajx['src'][:, -1:, :2]\n",
    "\n",
    "            x *= args.data_scale\n",
    "            y *= args.data_scale\n",
    "            x = x.double().cuda()\n",
    "            y = y.double().cuda()\n",
    "\n",
    "            x = x.view(-1, x.shape[1]*x.shape[2])\n",
    "            dest = y[:, sub_goal_indexes, :].detach().clone().view(y.size(0), -1)\n",
    "            future = y.view(y.size(0),-1)\n",
    "\n",
    "            dest_recon, mu, var, interpolated_future, cd= model.forward(x, dest=dest, mask=None, iteration=i)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            dest_loss, future_loss, kld, subgoal_reg = calculate_loss(dest, dest_recon, mu, var, criterion, future, interpolated_future, sub_goal_indexes)\n",
    "            loss = args.dest_loss_coeff * dest_loss + args.future_loss_coeff * future_loss + args.kld_coeff * kld  + cd*10 + subgoal_reg\n",
    "            # loss = args.dest_loss_coeff * dest_loss + args.future_loss_coeff * future_loss + args.kld_coeff * kld  + cd + subgoal_reg\n",
    "            # loss = dest_loss + future_loss + kld  + cd + subgoal_reg\n",
    "            loss.backward()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            total_dest_loss += dest_loss.item()\n",
    "            total_future_loss += future_loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % args.print_log == 0:\n",
    "                logger.info('{:5d}/{:5d} '.format(i, epoch) +\n",
    "                            'dest_loss={:8.6f} '.format(dest_loss.item()) +\n",
    "                            'future_loss={:8.6f} '.format(future_loss.item()) +\n",
    "                            'kld={:8.6f} '.format(kld.item()) +\n",
    "                            'cd={:8.6f} '.format(cd.item()) +\n",
    "                            'subgoal_reg={}'.format(subgoal_reg.detach().cpu().numpy())\n",
    "                )\n",
    "\n",
    "        return train_loss, total_dest_loss, total_future_loss\n",
    "\n",
    "    def test(model, dataloader, dataset, sub_goal_indexes, best_of_n=20):\n",
    "        model.eval()\n",
    "\n",
    "        total_dest_err = 0.\n",
    "        total_overall_err = 0.\n",
    "\n",
    "        for i, trajx in enumerate(dataloader):\n",
    "            x = trajx['src'][:, :, :2]\n",
    "            y = trajx['trg'][:, :, :2]\n",
    "            x = x - trajx['src'][:, -1:, :2]\n",
    "            y = y - trajx['src'][:, -1:, :2]\n",
    "\n",
    "            x *= args.data_scale\n",
    "            y *= args.data_scale\n",
    "            x = x.double().cuda()\n",
    "            y = y.double().cuda()\n",
    "\n",
    "\n",
    "\n",
    "            y = y.cpu().numpy()\n",
    "\n",
    "            x = x.view(-1, x.shape[1]*x.shape[2])\n",
    "\n",
    "            plan = y[:, sub_goal_indexes, :].reshape(y.shape[0],-1)\n",
    "            all_plan_errs = []\n",
    "            all_plans = []\n",
    "            for _ in range(best_of_n):\n",
    "                # dest_recon = model.forward(x, initial_pos, device=device)\n",
    "                # modes = torch.tensor(k % args.ny, device=device).long().repeat(batch_size)\n",
    "                plan_recon = model.forward(x, mask=None)\n",
    "                plan_recon = plan_recon.detach().cpu().numpy()\n",
    "                all_plans.append(plan_recon)\n",
    "                plan_err = np.linalg.norm(plan_recon - plan, axis=-1)\n",
    "                all_plan_errs.append(plan_err)\n",
    "\n",
    "            all_plan_errs = np.array(all_plan_errs) \n",
    "            all_plans = np.array(all_plans) \n",
    "            indices = np.argmin(all_plan_errs, axis=0)\n",
    "            best_plan = all_plans[indices, np.arange(x.shape[0]),  :]\n",
    "\n",
    "            # FDE\n",
    "            best_dest_err = np.linalg.norm(best_plan[:, -2:] - plan[:, -2:], axis=1).sum()\n",
    "\n",
    "            best_plan = torch.DoubleTensor(best_plan).cuda()\n",
    "            interpolated_future = model.predict(x, best_plan)\n",
    "            interpolated_future = interpolated_future.detach().cpu().numpy()\n",
    "\n",
    "            # ADE        \n",
    "            predicted_future = np.reshape(interpolated_future, (-1, args.future_length, 2))\n",
    "            overall_err = np.linalg.norm(y - predicted_future, axis=-1).mean(axis=-1).sum()\n",
    "\n",
    "            overall_err /= args.data_scale\n",
    "            best_dest_err /= args.data_scale\n",
    "\n",
    "            total_overall_err += overall_err\n",
    "            total_dest_err += best_dest_err\n",
    "\n",
    "        total_overall_err /= len(dataset)\n",
    "        total_dest_err /= len(dataset)\n",
    "\n",
    "\n",
    "        return total_overall_err, total_dest_err\n",
    "\n",
    "\n",
    "    def run_training(args):\n",
    "        model = LBEBM(\n",
    "            args.enc_past_size,\n",
    "            args.enc_dest_size,\n",
    "            args.enc_latent_size,\n",
    "            args.dec_size,\n",
    "            args.predictor_hidden_size,\n",
    "            args.fdim,\n",
    "            args.zdim,\n",
    "            args.sigma,\n",
    "            args.past_length,\n",
    "            args.future_length)\n",
    "        \n",
    "        model = model.double().cuda()\n",
    "        optimizer = optim.Adam(model.parameters(), lr= args.learning_rate)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_decay_step_size, gamma=args.lr_decay_gamma)\n",
    "\n",
    "\n",
    "\n",
    "        best_val_ade = 50\n",
    "        best_val_fde = 50\n",
    "        best_test_ade = 50\n",
    "        best_test_fde = 50\n",
    "\n",
    "        patience_epoch = 0\n",
    "\n",
    "        for epoch in range(args.num_epochs):\n",
    "            train_loss, dest_loss, overall_loss = train(model, optimizer, epoch, args.sub_goal_indexes)\n",
    "            overall_err, dest_err = test(model, val_dl, val_dataset, args.sub_goal_indexes, args.n_values)\n",
    "            test_overall_err, test_dest_err = test(model, test_dl, test_dataset, args.sub_goal_indexes, args.n_values)\n",
    "\n",
    "            patience_epoch += 1\n",
    "            if best_val_ade > overall_err:\n",
    "                patience_epoch = 0\n",
    "                best_val_ade = overall_err\n",
    "                best_val_fde = dest_err\n",
    "\n",
    "            if best_test_ade > test_overall_err:\n",
    "                best_test_ade = test_overall_err\n",
    "                best_test_fde = test_dest_err\n",
    "\n",
    "            logger.info(\"Train Loss {}\".format(train_loss))\n",
    "            logger.info(\"Overall Loss {}\".format(overall_loss))\n",
    "            logger.info(\"Dest Loss {}\".format(dest_loss))\n",
    "            logger.info(\"Val ADE {}\".format(overall_err))\n",
    "            logger.info(\"Val FDE {}\".format(dest_err))\n",
    "            logger.info(\"Val Best ADE {}\".format(best_val_ade))\n",
    "            logger.info(\"Val Best FDE {}\".format(best_val_fde))\n",
    "            logger.info(\"Test ADE {}\".format(test_overall_err))\n",
    "            logger.info(\"Test FDE {}\".format(test_dest_err))\n",
    "            logger.info(\"Test Best ADE {}\".format(best_test_ade))\n",
    "            logger.info(\"Test Best FDE {}\".format(best_test_fde))\n",
    "            logger.info(\"----->learning rate {}\".format(optimizer.param_groups[0]['lr'])) \n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "    def run_eval(args):\n",
    "        model = LBEBM(\n",
    "            args.enc_past_size,\n",
    "            args.enc_dest_size,\n",
    "            args.enc_latent_size,\n",
    "            args.dec_size,\n",
    "            args.predictor_hidden_size,\n",
    "            args.fdim,\n",
    "            args.zdim,\n",
    "            args.sigma,\n",
    "            args.past_length,\n",
    "            args.future_length)\n",
    "        \n",
    "        model = model.double().cuda()\n",
    "        \n",
    "        ckpt = torch.load(args.model_path, map_location=torch.device('cuda'))\n",
    "        model.load_state_dict(ckpt['model_state_dict'])\n",
    "        overall_err, dest_err = test(model, test_dl, test_dataset, args.sub_goal_indexes, args.n_values)\n",
    "        logger.info(\"Test ADE {}\".format(overall_err))\n",
    "        logger.info(\"Test FDE {}\".format(dest_err))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "        model = LBEBM(\n",
    "            args.enc_past_size,\n",
    "            args.enc_dest_size,\n",
    "            args.enc_latent_size,\n",
    "            args.dec_size,\n",
    "            args.predictor_hidden_size,\n",
    "            args.fdim,\n",
    "            args.zdim,\n",
    "            args.sigma,\n",
    "            args.past_length,\n",
    "            args.future_length)\n",
    "        \n",
    "        model = model.double().cuda()\n",
    "        optimizer = optim.Adam(model.parameters(), lr= args.learning_rate)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_decay_step_size, gamma=args.lr_decay_gamma)\n",
    "\n",
    "\n",
    "\n",
    "        best_val_ade = 50\n",
    "        best_val_fde = 50\n",
    "        best_test_ade = 50\n",
    "        best_test_fde = 50\n",
    "\n",
    "        patience_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-04 22:17:14,620 :     5/    0 dest_loss=7390.691125 future_loss=5959.156822 kld=250.303818 cd=4.296461 subgoal_reg=0.032897177516388224\n",
      "2023-11-04 22:17:16,071 :    11/    0 dest_loss=10010.119110 future_loss=8099.274887 kld=100.790721 cd=3.928837 subgoal_reg=0.2106618288487839\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47073/4103204886.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_goal_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_47073/3377673584.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, epoch, sub_goal_indexes)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0mtotal_dest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdest_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mtotal_future_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfuture_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch=0\n",
    "train_loss, dest_loss, overall_loss = train(model, optimizer, epoch, args.sub_goal_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "        dataloader=test_dl\n",
    "        dataset=test_dataset\n",
    "        sub_goal_indexes=args.sub_goal_indexes\n",
    "        best_of_n=20\n",
    "        model.eval()\n",
    "\n",
    "        total_dest_err = 0.\n",
    "        total_overall_err = 0.\n",
    "\n",
    "        for i, trajx in enumerate(test_dl):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "            x = trajx['src'][:, :, :2]\n",
    "            y = trajx['trg'][:, :, :2]\n",
    "            x = x - trajx['src'][:, -1:, :2]\n",
    "            y = y - trajx['src'][:, -1:, :2]\n",
    "\n",
    "            x *= args.data_scale\n",
    "            y *= args.data_scale\n",
    "            x = x.double().cuda()\n",
    "            y = y.double().cuda()\n",
    "\n",
    "\n",
    "\n",
    "            y = y.cpu().numpy()\n",
    "\n",
    "            x = x.view(-1, x.shape[1]*x.shape[2])\n",
    "\n",
    "            plan = y[:, sub_goal_indexes, :].reshape(y.shape[0],-1)\n",
    "            all_plan_errs = []\n",
    "            all_plans = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "            for _ in range(best_of_n):\n",
    "                # dest_recon = model.forward(x, initial_pos, device=device)\n",
    "                # modes = torch.tensor(k % args.ny, device=device).long().repeat(batch_size)\n",
    "                plan_recon = model.forward(x, mask=None)\n",
    "                plan_recon = plan_recon.detach().cpu().numpy()\n",
    "                all_plans.append(plan_recon)\n",
    "                plan_err = np.linalg.norm(plan_recon - plan, axis=-1)\n",
    "                all_plan_errs.append(plan_err)\n",
    "\n",
    "            all_plan_errs = np.array(all_plan_errs) \n",
    "            all_plans = np.array(all_plans) \n",
    "            indices = np.argmin(all_plan_errs, axis=0)\n",
    "            best_plan = all_plans[indices, np.arange(x.shape[0]),  :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([700, 8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(x, mask=None).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_target = torch.rand(4,8)\n",
    "plan_pred   = torch.rand(4,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plans     = torch.zeros(best_of_n,plan_target.size(0),plan_target.size(1))\n",
    "all_plan_errs = torch.zeros(best_of_n,plan_target.size(0),plan_target.size(1)//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(best_of_n):\n",
    "    all_plans[i]=torch.rand(4,8)\n",
    "    l2_distances_list = []\n",
    "    for idx in range(0, plan_target.shape[1], 2):\n",
    "        l2_distance = torch.norm(all_plans[i][:, idx:idx+2] - plan_target[:, idx:idx+2], dim=1)\n",
    "        l2_distances_list.append(l2_distance)\n",
    "    l2_distances = torch.stack(l2_distances_list, dim=1)\n",
    "    all_plan_errs[i]=l2_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_0_index=torch.argmin(all_plan_errs[:,:,0],axis=0) # torch.Size([4])\n",
    "# best_1_index=torch.argmin(all_plan_errs[:,:,1],axis=0) # torch.Size([4])\n",
    "# best_2_index=torch.argmin(all_plan_errs[:,:,2],axis=0) # torch.Size([4])\n",
    "# best_3_index=torch.argmin(all_plan_errs[:,:,3],axis=0) # torch.Size([4])\n",
    "\n",
    "# selected_fde=all_plan_errs[:, :, 3][torch.arange(best_3_index.size(0)), best_3_index]\n",
    "\n",
    "# selected_0 = all_plans[:, :, 0:2][best_0_index, torch.arange(best_0_index.size(0)), :]\n",
    "# selected_1 = all_plans[:, :, 2:4][best_1_index, torch.arange(best_1_index.size(0)), :]\n",
    "# selected_2 = all_plans[:, :, 4:6][best_2_index, torch.arange(best_2_index.size(0)), :]\n",
    "# selected_3 = all_plans[:, :, 6:8][best_3_index, torch.arange(best_3_index.size(0)), :]\n",
    "\n",
    "# selected_plan=torch.stack([selected_0,selected_1,selected_2,selected_3],dim=1)\n",
    "\n",
    "\n",
    "# selected_plans_list = []\n",
    "# selected_fde=all_plan_errs[:, :, 0]\n",
    "# # 循环计算每一个维度的最小误差并选择相应的计划\n",
    "# for i in range(all_plans.size(2)):\n",
    "#     best_index = torch.argmin(all_plan_errs[:,:,i], axis=0)\n",
    "#     selected_plan = all_plans[:, :, i][torch.arange(best_index.size(0)), best_index]\n",
    "#     selected_plans_list.append(selected_plan)\n",
    "#     if (i==all_plans.size(2)-1):\n",
    "#         selected_fde=all_plan_errs[:, :, i][torch.arange(best_index.size(0)), best_index]\n",
    "\n",
    "# # 将所有选择的计划堆叠起来\n",
    "# selected_plan = torch.stack(selected_plans_list, dim=1)\n",
    "\n",
    "\n",
    "selected_plans_list = []\n",
    "selected_plans_fde_list = []\n",
    "selected_fde=all_plan_errs[:, :, 0]\n",
    "for i in range(all_plan_errs.size(2)):\n",
    "    best_index = torch.argmin(all_plan_errs[:,:,i], axis=0)\n",
    "    selected_plan = all_plans[best_index, torch.arange(best_index.size(0)), 2*i:2*(i+1)]\n",
    "    selected_plans_list.append(selected_plan)\n",
    "\n",
    "    selected_fde=all_plan_errs[best_index,torch.arange(best_index.size(0)), i]\n",
    "    selected_plans_fde_list.append(selected_fde)\n",
    "\n",
    "# 将所有选择的计划堆叠起来\n",
    "selected_plan = torch.stack(selected_plans_list, dim=1)\n",
    "selected_plan = selected_plan.reshape(selected_plan.size(0),-1)\n",
    "selected_plans_fde=torch.stack(selected_plans_fde_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_plans_fde.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_future=torch.rand(4,24)\n",
    "y=torch.rand(4,12,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_future = interpolated_future.reshape(y.size(0),y.size(1),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_distance = torch.norm(interpolated_future - y, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_distances_plan_index=torch.tensor(args.sub_goal_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_distances_plan = l2_distance[:, selected_distances_plan_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_distances_plan.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_values = torch.min(selected_distances_plan, selected_plans_fde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_distance[:, selected_distances_plan_index].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_distance[:, selected_distances_plan_index]=min_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 12])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_distance.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (84435714.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_47073/84435714.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ade=torch.mean(l2_distance,dim=0,1)\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "ade=torch.sum(torch.mean(l2_distance,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ade.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "all_plan_errs = np.array(all_plan_errs) \n",
    "all_plans = np.array(all_plans) \n",
    "indices = np.argmin(all_plan_errs, axis=0)\n",
    "best_plan = all_plans[indices, 3,  :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 8, 8, 8, 8, 8, 8, 8],\n",
       "       [8, 8, 8, 8, 8, 8, 8, 8],\n",
       "       [8, 8, 8, 8, 8, 8, 8, 8],\n",
       "       [8, 8, 8, 8, 8, 8, 8, 8]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = y[:, args.sub_goal_indexes, :].reshape(y.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_target=plan.reshape(plan.size(0),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan=torch.rand(700,8).cpu().numpy()\n",
    "y=torch.rand(700,12,2).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "                plan_target=torch.tensor(plan).cuda()\n",
    "                gt=torch.tensor(y).cuda()\n",
    "\n",
    "                all_plans     = torch.zeros(best_of_n,plan_target.size(0),plan_target.size(1)).double().cuda()\n",
    "                all_plan_errs = torch.zeros(best_of_n,plan_target.size(0),plan_target.size(1)//2).double().cuda()\n",
    "                for i in range(best_of_n):\n",
    "                    plan_pred = model.forward(x, mask=None)\n",
    "                    all_plans[i]=plan_pred\n",
    "                    l2_distances_list = []\n",
    "                    for idx in range(0, plan_target.shape[1], 2):\n",
    "                        l2_distance = torch.norm(all_plans[i][:, idx:idx+2] - plan_target[:, idx:idx+2], dim=1)\n",
    "                        l2_distances_list.append(l2_distance)\n",
    "                    l2_distances = torch.stack(l2_distances_list, dim=1)\n",
    "                    all_plan_errs[i]=l2_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "                selected_plans_list = []\n",
    "                selected_plans_fde_list = []\n",
    "                for i in range(all_plan_errs.size(2)):\n",
    "                    best_index = torch.argmin(all_plan_errs[:,:,i], axis=0)\n",
    "                    selected_plan = all_plans[best_index, torch.arange(best_index.size(0)), 2*i:2*(i+1)]\n",
    "                    selected_plans_list.append(selected_plan)\n",
    "\n",
    "                    selected_fde=all_plan_errs[best_index,torch.arange(best_index.size(0)), i]\n",
    "                    selected_plans_fde_list.append(selected_fde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "                selected_plan = torch.stack(selected_plans_list, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_plans_fde=torch.stack(selected_plans_fde_list, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([700, 4])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_plans_fde.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TrafficPredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
