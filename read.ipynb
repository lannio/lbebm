{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module, Linear\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import datetime, shutil, argparse, logging, sys\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--seed', default=3, type=int)\n",
    "    parser.add_argument('--gpu_deterministic', type=bool, default=False, help='set cudnn in deterministic mode (slow)')\n",
    "    parser.add_argument(\"--data_scale\", default=60, type=float)\n",
    "    parser.add_argument(\"--dec_size\", default=[1024, 512, 1024], type=list)\n",
    "    parser.add_argument(\"--enc_dest_size\", default=[256, 128], type=list)\n",
    "    parser.add_argument(\"--enc_latent_size\", default=[256, 512], type=list)\n",
    "    parser.add_argument(\"--enc_past_size\", default=[512, 256], type=list)\n",
    "    parser.add_argument(\"--predictor_hidden_size\", default=[1024, 512, 256], type=list)\n",
    "    parser.add_argument(\"--non_local_theta_size\", default=[256, 128, 64], type=list)\n",
    "    parser.add_argument(\"--non_local_phi_size\", default=[256, 128, 64], type=list)\n",
    "    parser.add_argument(\"--non_local_g_size\", default=[256, 128, 64], type=list)\n",
    "    parser.add_argument(\"--non_local_dim\", default=128, type=int)\n",
    "    parser.add_argument(\"--fdim\", default=16, type=int)\n",
    "    parser.add_argument(\"--future_length\", default=12, type=int)\n",
    "    parser.add_argument(\"--device\", default=7, type=int)\n",
    "    parser.add_argument(\"--kld_coeff\", default=0.5, type=float)\n",
    "    parser.add_argument(\"--future_loss_coeff\", default=1, type=float)\n",
    "    parser.add_argument(\"--dest_loss_coeff\", default=2, type=float)\n",
    "    parser.add_argument(\"--learning_rate\", default=0.0001, type=float)\n",
    "    parser.add_argument(\"--lr_decay_step_size\", default=4, type=int)\n",
    "    parser.add_argument(\"--lr_decay_gamma\", default=0.5, type=float)\n",
    "    parser.add_argument(\"--mu\", default=0, type=float)\n",
    "    parser.add_argument(\"--n_values\", default=20, type=int)\n",
    "    parser.add_argument(\"--nonlocal_pools\", default=3, type=int)\n",
    "    parser.add_argument(\"--num_epochs\", default=100, type=int)\n",
    "    parser.add_argument(\"--num_workers\", default=0, type=int)\n",
    "    parser.add_argument(\"--past_length\", default=8, type=int)\n",
    "    parser.add_argument(\"--sigma\", default=1.3, type=float)\n",
    "    parser.add_argument(\"--zdim\", default=16, type=int)\n",
    "    parser.add_argument(\"--print_log\", default=6, type=int)\n",
    "    parser.add_argument(\"--sub_goal_indexes\", default=[2, 5, 8, 11], type=list)\n",
    "\n",
    "\n",
    "    parser.add_argument('--e_prior_sig', type=float, default=2, help='prior of ebm z')\n",
    "    parser.add_argument('--e_init_sig', type=float, default=2, help='sigma of initial distribution')\n",
    "    parser.add_argument('--e_activation', type=str, default='lrelu', choices=['gelu', 'lrelu', 'swish', 'mish'])\n",
    "    parser.add_argument('--e_activation_leak', type=float, default=0.2)\n",
    "    parser.add_argument('--e_energy_form', default='identity', choices=['identity', 'tanh', 'sigmoid', 'softplus'])\n",
    "    parser.add_argument('--e_l_steps', type=int, default=20, help='number of langevin steps')\n",
    "    parser.add_argument('--e_l_steps_pcd', type=int, default=20, help='number of langevin steps')\n",
    "    parser.add_argument('--e_l_step_size', type=float, default=0.4, help='stepsize of langevin')\n",
    "    parser.add_argument('--e_l_with_noise', default=True, type=bool, help='noise term of langevin')\n",
    "    parser.add_argument('--e_sn', default=False, type=bool, help='spectral regularization')\n",
    "    parser.add_argument('--e_lr', default=0.00003, type=float)\n",
    "    parser.add_argument('--e_is_grad_clamp', type=bool, default=False, help='whether doing the gradient clamp')\n",
    "    parser.add_argument('--e_max_norm', type=float, default=25, help='max norm allowed')\n",
    "    parser.add_argument('--e_decay', default=1e-4, help='weight decay for ebm')\n",
    "    parser.add_argument('--e_gamma', default=0.998, help='lr decay for ebm')\n",
    "    parser.add_argument('--e_beta1', default=0.9, type=float)\n",
    "    parser.add_argument('--e_beta2', default=0.999, type=float)\n",
    "    parser.add_argument('--memory_size', default=200000, type=int)\n",
    "\n",
    "\n",
    "    parser.add_argument('--dataset_name', type=str, default='eth')\n",
    "    parser.add_argument('--save_folder', type=str, default='1103_read/')\n",
    "    parser.add_argument('--dataset_folder', type=str, default='dataset')\n",
    "    parser.add_argument('--obs',type=int,default=8)\n",
    "    parser.add_argument('--preds',type=int,default=12)\n",
    "    parser.add_argument('--delim',type=str,default='\\t')\n",
    "    parser.add_argument('--verbose',action='store_true')\n",
    "    parser.add_argument('--val_size',type=int, default=0)\n",
    "    parser.add_argument('--batch_size',type=int,default=70)\n",
    "\n",
    "    parser.add_argument('--ny', type=int, default=1)\n",
    "    # parser.add_argument('--model_path', type=str, default='saved_models/lbebm_eth.pt')\n",
    "    parser.add_argument('--model_path', type=str, default=None)\n",
    "\n",
    "\n",
    "    return parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_gpu(gpu):\n",
    "    torch.cuda.set_device('cuda:{}'.format(gpu))\n",
    "\n",
    "def get_exp_id(file):\n",
    "    return os.path.splitext(os.path.basename(file))[0]\n",
    "\n",
    "def get_output_dir(exp_id):\n",
    "    t = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    output_dir = os.path.join('output/' + exp_id, t)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "def setup_logging(name, output_dir, console=True):\n",
    "    log_format = logging.Formatter(\"%(asctime)s : %(message)s\")\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.handlers = []\n",
    "    output_file = os.path.join(output_dir, 'output.log')\n",
    "    file_handler = logging.FileHandler(output_file)\n",
    "    file_handler.setFormatter(log_format)\n",
    "    logger.addHandler(file_handler)\n",
    "    if console:\n",
    "        console_handler = logging.StreamHandler(sys.stdout)\n",
    "        console_handler.setFormatter(log_format)\n",
    "        logger.addHandler(console_handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    return logger\n",
    "\n",
    "def set_cuda(deterministic=True):\n",
    "    if torch.cuda.is_available():\n",
    "        if not deterministic:\n",
    "            torch.backends.cudnn.deterministic = False\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        else:\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def set_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def copy_source(file, output_dir):\n",
    "    shutil.copyfile(file, os.path.join(output_dir, os.path.basename(file)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    args = parse_args()\n",
    "    # custom\n",
    "    if(args.dataset_name=='eth'):\n",
    "        args.seed=3\n",
    "        args.kld_coeff=0.5\n",
    "        args.lr_decay_step_size=4\n",
    "    elif(args.dataset_name=='hotel'):\n",
    "        args.seed=2\n",
    "        args.kld_coeff=0.8\n",
    "        args.lr_decay_step_size=1\n",
    "    elif(args.dataset_name=='univ'): \n",
    "        args.seed=1\n",
    "        args.kld_coeff=0.5\n",
    "        args.lr_decay_step_size=30\n",
    "    elif(args.dataset_name=='zara1'): \n",
    "        args.seed=1\n",
    "        args.kld_coeff=0.5\n",
    "        args.lr_decay_step_size=30\n",
    "    elif(args.dataset_name=='zara2'): \n",
    "        args.seed=1\n",
    "        args.kld_coeff=0.5\n",
    "        args.lr_decay_step_size=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-03 22:42:30,423 : Namespace(batch_size=70, data_scale=60, dataset_folder='dataset', dataset_name='eth', dec_size=[1024, 512, 1024], delim='\\t', dest_loss_coeff=2, device=7, e_activation='lrelu', e_activation_leak=0.2, e_beta1=0.9, e_beta2=0.999, e_decay=0.0001, e_energy_form='identity', e_gamma=0.998, e_init_sig=2, e_is_grad_clamp=False, e_l_step_size=0.4, e_l_steps=20, e_l_steps_pcd=20, e_l_with_noise=True, e_lr=3e-05, e_max_norm=25, e_prior_sig=2, e_sn=False, enc_dest_size=[256, 128], enc_latent_size=[256, 512], enc_past_size=[512, 256], fdim=16, future_length=12, future_loss_coeff=1, gpu_deterministic=False, kld_coeff=0.5, learning_rate=0.0001, lr_decay_gamma=0.5, lr_decay_step_size=4, memory_size=200000, model_path=None, mu=0, n_values=20, non_local_dim=128, non_local_g_size=[256, 128, 64], non_local_phi_size=[256, 128, 64], non_local_theta_size=[256, 128, 64], nonlocal_pools=3, num_epochs=100, num_workers=0, ny=1, obs=8, past_length=8, predictor_hidden_size=[1024, 512, 256], preds=12, print_log=6, save_folder='1103_read/', seed=3, sigma=1.3, sub_goal_indexes=[2, 5, 8, 11], val_size=0, verbose=False, way_points=[0, 1, 3, 4, 6, 7, 9, 10], zdim=16)\n",
      "start loading dataset\n",
      "validation set size -> 0\n",
      "loaded preprocessed data file dataset/eth/train/saved_data.pickle\n",
      "start loading dataset\n",
      "validation set size -> 0\n",
      "loaded preprocessed data file dataset/eth/val/saved_data.pickle\n",
      "start loading dataset\n",
      "validation set size -> 0\n",
      "loaded preprocessed data file dataset/eth/test/saved_data.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/scratch/yaoliu/anaconda3/envs/TrafficPredict/lib/python3.8/site-packages/torch/cuda/__init__.py:106: UserWarning: \n",
      "NVIDIA RTX A5000 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA RTX A5000 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "    output_dir='/home/yaoliu/scratch/experiment/lbebm/'+args.save_folder + args.dataset_name\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    # copy_source(__file__, output_dir)\n",
    "    set_gpu(args.device)\n",
    "    set_cuda(deterministic=args.gpu_deterministic)\n",
    "    set_seed(args.seed)\n",
    "    args.way_points = list(set(list(range(args.future_length))) - set(args.sub_goal_indexes))\n",
    "\n",
    "    logger = setup_logging('job{}'.format(0), output_dir, console=True)\n",
    "    logger.info(args)\n",
    "\n",
    "    if args.val_size==0:\n",
    "        train_dataset, _ = utils.create_dataset(args.dataset_folder,args.dataset_name,0,args.obs,args.preds,delim=args.delim,train=True, verbose=True)\n",
    "        val_dataset, _ = utils.create_dataset(args.dataset_folder,args.dataset_name,0,args.obs,args.preds,delim=args.delim,train=False, verbose=True)\n",
    "    else:\n",
    "        train_dataset, val_dataset = utils.create_dataset(args.dataset_folder, args.dataset_name, args.val_size,args.obs, args.preds, delim=args.delim, train=True, verbose=args.verbose)\n",
    "\n",
    "    test_dataset, _ =  utils.create_dataset(args.dataset_folder,args.dataset_name,0,args.obs,args.preds,delim=args.delim,train=False,eval=True, verbose=True)\n",
    "\n",
    "    tr_dl = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
    "    val_dl = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size*10, shuffle=False, num_workers=0)\n",
    "    test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size*10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def initial_pos(traj_batches):\n",
    "        batches = []\n",
    "        for b in traj_batches:\n",
    "            starting_pos = b[:,7,:].copy()/1000\n",
    "            batches.append(starting_pos)\n",
    "        return batches\n",
    "\n",
    "    def sample_p_0(n, nz=16):\n",
    "        return args.e_init_sig * torch.randn(*[n, nz]).double().cuda()\n",
    "\n",
    "    def calculate_loss(dest, dest_recon, mean, log_var, criterion, future, interpolated_future, sub_goal_indexes):\n",
    "        dest_loss = criterion(dest, dest_recon)\n",
    "        future_loss = criterion(future, interpolated_future)\n",
    "        subgoal_reg = criterion(dest_recon, interpolated_future.view(dest.size(0), future.size(1)//2, 2)[:, sub_goal_indexes, :].view(dest.size(0), -1))\n",
    "        kl = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "        return dest_loss, future_loss, kl, subgoal_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class ConcatSquashLinear(nn.Module):\n",
    "        def __init__(self, dim_in, dim_out, dim_ctx):\n",
    "            super(ConcatSquashLinear, self).__init__()\n",
    "            self._layer = Linear(dim_in, dim_out)\n",
    "            self._hyper_bias = Linear(dim_ctx, dim_out, bias=False)\n",
    "            self._hyper_gate = Linear(dim_ctx, dim_out)\n",
    "\n",
    "        def forward(self, ctx, x):\n",
    "            gate = torch.sigmoid(self._hyper_gate(ctx))\n",
    "            bias = self._hyper_bias(ctx)\n",
    "            # if x.dim() == 3:\n",
    "            #     gate = gate.unsqueeze(1)\n",
    "            #     bias = bias.unsqueeze(1)\n",
    "            ret = self._layer(x) * gate + bias\n",
    "            return ret        \n",
    "\n",
    "    class PositionalEncoding(nn.Module):\n",
    "        def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "            super().__init__()\n",
    "\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "            pe = torch.zeros(max_len, d_model)\n",
    "            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "            div_term = torch.exp(\n",
    "                torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "            )\n",
    "            pe[:, 0::2] = torch.sin(position * div_term)\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "            pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "            self.register_buffer(\"pe\", pe)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x + self.pe[: x.size(0), :]\n",
    "            return self.dropout(x)\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, hidden_size=(1024, 512), activation='relu', discrim=False, dropout=-1):\n",
    "            super(MLP, self).__init__()\n",
    "            dims = []\n",
    "            dims.append(input_dim)\n",
    "            dims.extend(hidden_size)\n",
    "            dims.append(output_dim)\n",
    "            self.layers = nn.ModuleList()\n",
    "            for i in range(len(dims)-1):\n",
    "                self.layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "\n",
    "            if activation == 'relu':\n",
    "                self.activation = nn.ReLU()\n",
    "            elif activation == 'sigmoid':\n",
    "                self.activation = nn.Sigmoid()\n",
    "\n",
    "            self.sigmoid = nn.Sigmoid() if discrim else None\n",
    "            self.dropout = dropout\n",
    "\n",
    "        def forward(self, x):\n",
    "            for i in range(len(self.layers)):\n",
    "                x = self.layers[i](x)\n",
    "                if i != len(self.layers)-1:\n",
    "                    x = self.activation(x)\n",
    "                    if self.dropout != -1:\n",
    "                        x = nn.Dropout(min(0.1, self.dropout/3) if i == 1 else self.dropout)(x)\n",
    "                elif self.sigmoid:\n",
    "                    x = self.sigmoid(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    class ReplayMemory(object):\n",
    "        def __init__(self, capacity):\n",
    "            self.capacity = capacity\n",
    "            self.memory = []\n",
    "            self.position = 0\n",
    "\n",
    "        def push(self, input_memory):\n",
    "            if len(self.memory) < self.capacity:\n",
    "                self.memory.append(None)\n",
    "            self.memory[self.position] = input_memory\n",
    "            self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "        def sample(self, n=100):\n",
    "            samples = random.sample(self.memory, n)\n",
    "            return torch.cat(samples)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.memory)\n",
    "    \n",
    "    def extract(a, t, x_shape):\n",
    "        batch_size = t.shape[0]\n",
    "        out = a.to(t.device).gather(0, t).float()\n",
    "        out = out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class LBEBM(nn.Module):\n",
    "        def __init__(self, \n",
    "                    enc_past_size, \n",
    "                    enc_dest_size, \n",
    "                    enc_latent_size, \n",
    "                    dec_size, \n",
    "                    predictor_size, \n",
    "                    fdim, \n",
    "                    zdim, \n",
    "                    sigma, \n",
    "                    past_length, \n",
    "                    future_length):\n",
    "            super(LBEBM, self).__init__()\n",
    "\n",
    "            # var_sched\n",
    "            self.num_steps = 100\n",
    "            self.beta_1 = 1e-4\n",
    "            self.beta_T = 5e-2\n",
    "            self.mode = 'linear'\n",
    "            self.cosine_s=8e-3\n",
    "\n",
    "            if self.mode == 'linear':\n",
    "                betas = torch.linspace(self.beta_1, self.beta_T, steps=self.num_steps)\n",
    "            elif self.mode == 'cosine':\n",
    "                timesteps = (\n",
    "                torch.arange(self.num_steps + 1) / self.num_steps + self.cosine_s\n",
    "                )\n",
    "                alphas = timesteps / (1 + self.cosine_s) * math.pi / 2\n",
    "                alphas = torch.cos(alphas).pow(2)\n",
    "                alphas = alphas / alphas[0]\n",
    "                betas = 1 - alphas[1:] / alphas[:-1]\n",
    "                betas = betas.clamp(max=0.999)\n",
    "\n",
    "            betas = torch.cat([torch.zeros([1]), betas], dim=0)     # Padding\n",
    "\n",
    "            alphas = 1 - betas\n",
    "            log_alphas = torch.log(alphas)\n",
    "            for i in range(1, log_alphas.size(0)):  # 1 to T\n",
    "                log_alphas[i] += log_alphas[i - 1]\n",
    "            alpha_bars = log_alphas.exp()\n",
    "\n",
    "            sigmas_flex = torch.sqrt(betas)\n",
    "            sigmas_inflex = torch.zeros_like(sigmas_flex)\n",
    "            for i in range(1, sigmas_flex.size(0)):\n",
    "                sigmas_inflex[i] = ((1 - alpha_bars[i-1]) / (1 - alpha_bars[i])) * betas[i]\n",
    "            sigmas_inflex = torch.sqrt(sigmas_inflex)\n",
    "\n",
    "            self.register_buffer('betas', betas)\n",
    "            self.register_buffer('alphas', alphas)\n",
    "            self.register_buffer('alpha_bars', alpha_bars)\n",
    "            self.register_buffer('sigmas_flex', sigmas_flex)\n",
    "            self.register_buffer('sigmas_inflex', sigmas_inflex)\n",
    "\n",
    "            # backbone\n",
    "            # point_dim=2\n",
    "            context_dim=256\n",
    "            tf_layer=3\n",
    "            residual=False\n",
    "\n",
    "            self.residual = residual\n",
    "            self.pos_emb = PositionalEncoding(d_model=4, dropout=0.1, max_len=20)\n",
    "            self.concat1 = ConcatSquashLinear(zdim,2*zdim,fdim+3)\n",
    "            self.layer = nn.TransformerEncoderLayer(d_model=4, nhead=4, dim_feedforward=zdim)\n",
    "            self.transformer_encoder = nn.TransformerEncoder(self.layer, num_layers=tf_layer)\n",
    "            self.concat3 = ConcatSquashLinear(2*zdim,4*zdim,fdim+3)\n",
    "            self.concat4 = ConcatSquashLinear(4*zdim,2*zdim,fdim+3)\n",
    "            self.linear = ConcatSquashLinear(2*zdim, zdim, fdim+3)\n",
    "\n",
    "\n",
    "            self.zdim = zdim\n",
    "            self.sigma = sigma\n",
    "            self.nonlocal_pools = args.nonlocal_pools\n",
    "            non_local_dim = args.non_local_dim\n",
    "            non_local_phi_size = args.non_local_phi_size\n",
    "            non_local_g_size = args.non_local_g_size\n",
    "            non_local_theta_size = args.non_local_theta_size\n",
    "\n",
    "            self.encoder_past = MLP(input_dim=past_length*2, output_dim=fdim, hidden_size=enc_past_size)\n",
    "            self.encoder_dest = MLP(input_dim=len(args.sub_goal_indexes)*2, output_dim=fdim, hidden_size=enc_dest_size)\n",
    "            self.encoder_latent = MLP(input_dim=2*fdim, output_dim=2*zdim, hidden_size=enc_latent_size)\n",
    "            self.decoder = MLP(input_dim=fdim+zdim, output_dim=len(args.sub_goal_indexes)*2, hidden_size=dec_size)\n",
    "            self.predictor = MLP(input_dim=2*fdim, output_dim=2*(future_length), hidden_size=predictor_size)\n",
    "\n",
    "            self.non_local_theta = MLP(input_dim = fdim, output_dim = non_local_dim, hidden_size=non_local_theta_size)\n",
    "            self.non_local_phi = MLP(input_dim = fdim, output_dim = non_local_dim, hidden_size=non_local_phi_size)\n",
    "            self.non_local_g = MLP(input_dim = fdim, output_dim = fdim, hidden_size=non_local_g_size)\n",
    "\n",
    "            self.EBM = nn.Sequential(\n",
    "                nn.Linear(zdim + fdim, 200),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(200, 200),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(200, args.ny),\n",
    "                )\n",
    "                        \n",
    "            self.replay_memory = ReplayMemory(args.memory_size)\n",
    "\n",
    "        def forward(self, x, dest=None, mask=None, iteration=1, y=None):\n",
    "            # x torch.Size([70, 16])\n",
    "            ftraj = self.encoder_past(x) # torch.Size([70, 16])\n",
    "\n",
    "            if mask:\n",
    "                for _ in range(self.nonlocal_pools):\n",
    "                    ftraj = self.non_local_social_pooling(ftraj, mask)\n",
    "\n",
    "            if self.training:\n",
    "                dest_features = model.encoder_dest(dest) # torch.Size([70, 16])\n",
    "                features = torch.cat((ftraj, dest_features), dim=1) # torch.Size([70, 32])\n",
    "                latent =  model.encoder_latent(features) # torch.Size([70, 32])\n",
    "                mu = latent[:, 0:model.zdim]\n",
    "                logvar = latent[:, model.zdim:]\n",
    "\n",
    "                var = logvar.mul(0.5).exp_()\n",
    "                eps = torch.DoubleTensor(var.size()).normal_().cuda()\n",
    "                z_g_k = eps.mul(var).add_(mu)\n",
    "                z_g_k = z_g_k.double().cuda() # torch.Size([70, 16])\n",
    "\n",
    "            if self.training:\n",
    "                # pcd = True if len(self.replay_memory) == args.memory_size else False\n",
    "                # if pcd:\n",
    "                #     z_e_0 = self.replay_memory.sample(n=ftraj.size(0)).clone().detach().cuda()\n",
    "                # else:\n",
    "                #     z_e_0 = sample_p_0(n=ftraj.size(0), nz=self.zdim)\n",
    "                # z_e_k, _ = self.sample_langevin_prior_z(Variable(z_e_0), ftraj, pcd=pcd, verbose=(iteration % 1000==0))\n",
    "                # for _z_e_k in z_e_k.clone().detach().cpu().split(1):\n",
    "                #     self.replay_memory.push(_z_e_k)\n",
    "                z_e_k = z_g_k\n",
    "            else:\n",
    "                # z_e_0 = sample_p_0(n=ftraj.size(0), nz=self.zdim) # torch.Size([70, 16])\n",
    "                z_e_k = self.diffusion_sample(ftraj)\n",
    "                # z_e_k, _ = self.sample_langevin_prior_z(Variable(z_e_0), ftraj, pcd=False, verbose=(iteration % 1000==0), y=y)                        \n",
    "            z_e_k = z_e_k.double().cuda()\n",
    "\n",
    "            if self.training:\n",
    "                decoder_input = torch.cat((ftraj, z_g_k), dim=1)\n",
    "            else:\n",
    "                decoder_input = torch.cat((ftraj, z_e_k), dim=1)\n",
    "            generated_dest = self.decoder(decoder_input)\n",
    "\n",
    "            if self.training:\n",
    "                generated_dest_features = self.encoder_dest(generated_dest)\n",
    "                prediction_features = torch.cat((ftraj, generated_dest_features), dim=1)\n",
    "                pred_future = self.predictor(prediction_features)\n",
    "\n",
    "                # en_pos = self.ebm(z_g_k, ftraj).mean() # torch.Size([70]) mean\n",
    "                # en_neg = self.ebm(z_e_k.detach().clone(), ftraj).mean()\n",
    "                # cd = en_pos - en_neg\n",
    "                cd = self.diffusion_loss(z_g_k, ftraj)\n",
    "                return generated_dest, mu, logvar, pred_future, cd, en_pos, en_neg, pcd\n",
    "\n",
    "            return generated_dest\n",
    "\n",
    "        def diffusion_loss(self, x_0, context, t=None):\n",
    "            # x_0 70,16\n",
    "\n",
    "            batch_size, point_dim = x_0.size()\n",
    "            if t == None:\n",
    "                t = self.uniform_sample_t(batch_size)\n",
    "\n",
    "            alpha_bar = self.alpha_bars[t]\n",
    "            beta = self.betas[t].cuda()\n",
    "\n",
    "            c0 = torch.sqrt(alpha_bar).view(-1, 1).cuda()       # (B, 1, 1)\n",
    "            c1 = torch.sqrt(1 - alpha_bar).view(-1, 1).cuda()   # (B, 1, 1)\n",
    "\n",
    "            e_rand = torch.randn_like(x_0).cuda()  # (B, N, d)\n",
    "\n",
    "\n",
    "            e_theta = self.net(c0 * x_0 + c1 * e_rand, beta=beta, context=context)\n",
    "            loss = F.mse_loss(e_theta.view(-1, point_dim), e_rand.view(-1, point_dim), reduction='mean')\n",
    "            return loss\n",
    "\n",
    "        def diffusion_sample(self, context):\n",
    "\n",
    "            self.alphas_cumprod = self.alpha_bars\n",
    "\n",
    "            batch_size = context.size(0)\n",
    "\n",
    "            ddim_timesteps=10\n",
    "            ddim_eta=0.0\n",
    "            clip_denoised=False\n",
    "\n",
    "            c = self.num_steps // ddim_timesteps\n",
    "            ddim_timestep_seq = np.asarray(list(range(0, self.num_steps, c)))\n",
    "            # add one to get the final alpha values right (the ones from first scale to data during sampling)\n",
    "            ddim_timestep_seq = ddim_timestep_seq + 1\n",
    "            # previous sequence\n",
    "            ddim_timestep_prev_seq = np.append(np.array([0]), ddim_timestep_seq[:-1])\n",
    "\n",
    "            # sample_img = torch.randn([batch_size, self.zdim]).to(context.device)\n",
    "            sample_img = sample_p_0(n=ftraj.size(0), nz=self.zdim)\n",
    "\n",
    "\n",
    "            for i in reversed(range(0, ddim_timesteps)) :\n",
    "                t = torch.full((batch_size,), ddim_timestep_seq[i], device=context.device, dtype=torch.long)\n",
    "                prev_t = torch.full((batch_size,), ddim_timestep_prev_seq[i], device=context.device, dtype=torch.long)\n",
    "                \n",
    "                # 1. get current and previous alpha_cumprod\n",
    "                \n",
    "                alpha_cumprod_t = extract(self.alphas_cumprod, t, sample_img.shape)\n",
    "                alpha_cumprod_t_prev = extract(self.alphas_cumprod, prev_t, sample_img.shape)\n",
    "        \n",
    "                # 2. predict noise using model\n",
    "                beta = self.betas[[t[0].item()]*batch_size]\n",
    "                pred_noise = self.net(sample_img, beta=beta, context=context)\n",
    "                \n",
    "                # 3. get the predicted x_0\n",
    "                pred_x0 = (sample_img - torch.sqrt((1. - alpha_cumprod_t)) * pred_noise) / torch.sqrt(alpha_cumprod_t)\n",
    "                if clip_denoised:\n",
    "                    pred_x0 = torch.clamp(pred_x0, min=-1., max=1.)\n",
    "                \n",
    "                # 4. compute variance: \"sigma_t(η)\" -> see formula (16)\n",
    "                # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)\n",
    "                sigmas_t = ddim_eta * torch.sqrt(\n",
    "                    (1 - alpha_cumprod_t_prev) / (1 - alpha_cumprod_t) * (1 - alpha_cumprod_t / alpha_cumprod_t_prev))\n",
    "                \n",
    "                # 5. compute \"direction pointing to x_t\" of formula (12)\n",
    "                pred_dir_xt = torch.sqrt(1 - alpha_cumprod_t_prev - sigmas_t**2) * pred_noise\n",
    "                \n",
    "                # 6. compute x_{t-1} of formula (12)\n",
    "                x_prev = torch.sqrt(alpha_cumprod_t_prev) * pred_x0 + pred_dir_xt + sigmas_t * torch.randn_like(sample_img)\n",
    "\n",
    "                sample_img = x_prev.detach()\n",
    "\n",
    "            return sample_img\n",
    "\n",
    "        def net(self, x, beta, context):\n",
    "            batch_size = x.size(0)\n",
    "            beta = beta.view(batch_size, 1)          # (B, 1, 1)\n",
    "            context = context.view(batch_size, -1)   # (B, 1, F)\n",
    "\n",
    "            time_emb = torch.cat([beta, torch.sin(beta), torch.cos(beta)], dim=-1)  # (B, 1, 3)\n",
    "            ctx_emb = torch.cat([time_emb, context], dim=-1)    # (B, 1, F+3)\n",
    "            x = self.concat1(ctx_emb,x)\n",
    "            final_emb = x.reshape(x.size()[0],8,-1).permute(1,0,2)\n",
    "\n",
    "            final_emb = self.pos_emb(final_emb)\n",
    "\n",
    "            trans = self.transformer_encoder(final_emb).permute(1,0,2).reshape(x.size()[0],-1)\n",
    "\n",
    "            trans = self.concat3(ctx_emb, trans)\n",
    "            trans = self.concat4(ctx_emb, trans)\n",
    "            return self.linear(ctx_emb, trans)\n",
    "\n",
    "        def uniform_sample_t(self, batch_size):\n",
    "            ts = np.random.choice(np.arange(1, self.num_steps+1), batch_size)\n",
    "            return ts.tolist()\n",
    "\n",
    "        def get_sigmas(self, t, flexibility):\n",
    "            assert 0 <= flexibility and flexibility <= 1\n",
    "            sigmas = self.sigmas_flex[t] * flexibility + self.sigmas_inflex[t] * (1 - flexibility)\n",
    "            return sigmas\n",
    "\n",
    "        def ebm(self, z, condition, cls_output=False):\n",
    "            condition_encoding = condition.detach().clone()\n",
    "            z_c = torch.cat((z, condition_encoding), dim=1)\n",
    "            conditional_neg_energy = self.EBM(z_c)\n",
    "            assert conditional_neg_energy.shape == (z.size(0), args.ny)\n",
    "            if cls_output:\n",
    "                return - conditional_neg_energy\n",
    "            else:\n",
    "                return - conditional_neg_energy.logsumexp(dim=1)\n",
    "        \n",
    "        def sample_langevin_prior_z(self, z, condition, pcd=False, verbose=False, y=None):\n",
    "            z = z.clone().detach()\n",
    "            z.requires_grad = True\n",
    "            _e_l_steps = args.e_l_steps_pcd if pcd else args.e_l_steps\n",
    "            _e_l_step_size = args.e_l_step_size\n",
    "            for i in range(_e_l_steps):\n",
    "                if y is None:\n",
    "                    en = self.ebm(z, condition)\n",
    "                else:\n",
    "                    en = self.ebm(z, condition, cls_output=True)[range(z.size(0)), y]\n",
    "                z_grad = torch.autograd.grad(en.sum(), z)[0]\n",
    "\n",
    "                z.data = z.data - 0.5 * _e_l_step_size * _e_l_step_size * (z_grad + 1.0 / (args.e_prior_sig * args.e_prior_sig) * z.data)\n",
    "                if args.e_l_with_noise:\n",
    "                    z.data += _e_l_step_size * torch.randn_like(z).data\n",
    "\n",
    "                if (i % 5 == 0 or i == _e_l_steps - 1) and verbose:\n",
    "                    if y is None:\n",
    "                        print('Langevin prior {:3d}/{:3d}: energy={:8.3f}'.format(i+1, _e_l_steps, en.sum().item()))\n",
    "                    else:\n",
    "                        logger.info('Conditional Langevin prior {:3d}/{:3d}: energy={:8.3f}'.format(i + 1, _e_l_steps, en.sum().item()))\n",
    "\n",
    "                z_grad_norm = z_grad.view(z_grad.size(0), -1).norm(dim=1).mean()\n",
    "\n",
    "            return z.detach(), z_grad_norm\n",
    "\n",
    "\n",
    "        def predict(self, past, generated_dest):\n",
    "            ftraj = self.encoder_past(past)\n",
    "            generated_dest_features = self.encoder_dest(generated_dest)\n",
    "            prediction_features = torch.cat((ftraj, generated_dest_features), dim=1)\n",
    "            interpolated_future = self.predictor(prediction_features)\n",
    "\n",
    "            return interpolated_future\n",
    "\n",
    "\n",
    "        def non_local_social_pooling(self, feat, mask):\n",
    "            theta_x = self.non_local_theta(feat)\n",
    "            phi_x = self.non_local_phi(feat).transpose(1,0)\n",
    "            f = torch.matmul(theta_x, phi_x)\n",
    "            f_weights = F.softmax(f, dim = -1)\n",
    "            f_weights = f_weights * mask\n",
    "            f_weights = F.normalize(f_weights, p=1, dim=1)\n",
    "            pooled_f = torch.matmul(f_weights, self.non_local_g(feat))\n",
    "\n",
    "            return pooled_f + feat\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "        model = LBEBM(\n",
    "            args.enc_past_size,\n",
    "            args.enc_dest_size,\n",
    "            args.enc_latent_size,\n",
    "            args.dec_size,\n",
    "            args.predictor_hidden_size,\n",
    "            args.fdim,\n",
    "            args.zdim,\n",
    "            args.sigma,\n",
    "            args.past_length,\n",
    "            args.future_length)\n",
    "        \n",
    "        model = model.double().cuda()\n",
    "        optimizer = optim.Adam(model.parameters(), lr= args.learning_rate)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_decay_step_size, gamma=args.lr_decay_gamma)\n",
    "\n",
    "\n",
    "\n",
    "        best_val_ade = 50\n",
    "        best_val_fde = 50\n",
    "        best_test_ade = 50\n",
    "        best_test_fde = 50\n",
    "\n",
    "        patience_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=0\n",
    "sub_goal_indexes=args.sub_goal_indexes\n",
    "model.train()\n",
    "train_loss, total_dest_loss, total_future_loss = 0, 0, 0\n",
    "criterion = nn.MSELoss()\n",
    "for i, trajx in enumerate(tr_dl):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trajx['src'][:, :, :2]\n",
    "y = trajx['trg'][:, :, :2]\n",
    "x = x - trajx['src'][:, -1:, :2]\n",
    "y = y - trajx['src'][:, -1:, :2]\n",
    "\n",
    "x *= args.data_scale\n",
    "y *= args.data_scale\n",
    "x = x.double().cuda()\n",
    "y = y.double().cuda()\n",
    "\n",
    "x = x.view(-1, x.shape[1]*x.shape[2])\n",
    "dest = y[:, sub_goal_indexes, :].detach().clone().view(y.size(0), -1)\n",
    "future = y.view(y.size(0),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=None\n",
    "iteration=i\n",
    "y=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftraj = model.encoder_past(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_features = model.encoder_dest(dest) # torch.Size([70, 16])\n",
    "features = torch.cat((ftraj, dest_features), dim=1) # torch.Size([70, 32])\n",
    "latent =  model.encoder_latent(features) # torch.Size([70, 32])\n",
    "mu = latent[:, 0:model.zdim]\n",
    "logvar = latent[:, model.zdim:]\n",
    "\n",
    "var = logvar.mul(0.5).exp_()\n",
    "eps = torch.DoubleTensor(var.size()).normal_().cuda()\n",
    "z_g_k = eps.mul(var).add_(mu)\n",
    "z_g_k = z_g_k.double().cuda() # torch.Size([70, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_e_0 = sample_p_0(n=ftraj.size(0), nz=model.zdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langevin prior   1/ 20: energy=  15.345\n",
      "Langevin prior   6/ 20: energy=  14.872\n",
      "Langevin prior  11/ 20: energy=  14.884\n",
      "Langevin prior  16/ 20: energy=  15.137\n",
      "Langevin prior  20/ 20: energy=  15.198\n"
     ]
    }
   ],
   "source": [
    "z_e_k, _ = model.sample_langevin_prior_z(Variable(z_e_0), ftraj, pcd=False, verbose=(iteration % 1000==0), y=y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_pos = model.ebm(z_g_k, ftraj).mean() # torch.Size([70]) mean\n",
    "en_neg = model.ebm(z_e_k.detach().clone(), ftraj).mean()\n",
    "cd = en_pos - en_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = model.diffusion_loss(z_g_k, ftraj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_e_0.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_e_k = model.diffusion_sample(ftraj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 16])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "z_e_k.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TrafficPredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
